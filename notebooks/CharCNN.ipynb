{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharCNN\n",
    "\n",
    "Initial exploration and implementation of a character-level convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 464736 messages\n",
      "Loaded 467713 messages\n",
      "Loaded 464743 messages\n"
     ]
    }
   ],
   "source": [
    "def txt_to_list(txt_file, delimiter=chr(0x06)):\n",
    "    with open(txt_file) as f:\n",
    "        msg_reader = csv.reader(f, delimiter=delimiter)\n",
    "\n",
    "        messages = []\n",
    "        for msg in msg_reader:\n",
    "            messages.extend(msg)\n",
    "\n",
    "    print(f'Loaded {len(messages)} messages')\n",
    "    \n",
    "    return messages\n",
    "\n",
    "delimiter = chr(0x06)\n",
    "\n",
    "raw_txt = '../data/raw_messages/raw_sat_plaintext_1.txt'\n",
    "messages = load_txt(raw_txt, delimiter)\n",
    "\n",
    "ceasar_txt = '../data/en_messages/en_plaintext_1_ceasar.txt'\n",
    "messages_en = load_txt(ceasar_txt)\n",
    "\n",
    "columnar_txt = '../data/en_messages/en_plaintext_1_columnar.txt'\n",
    "messages_en.extend(load_txt(columnar_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get alphabet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet:\n",
      "\u0002\u0006\u0007\t\n",
      "\u000b",
      "\f",
      "\u0019\u001c",
      "\u001d",
      "\u001f !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|~\n"
     ]
    }
   ],
   "source": [
    "alphabet=\"\"\n",
    "with open(raw_txt, 'r') as fd:\n",
    "    alphabet = ''.join(sorted(list(set(fd.read()))))\n",
    "\n",
    "num_characters = len(alphabet)\n",
    "\n",
    "print(f'Alphabet:\\n{alphabet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get character counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {c:0 for c in alphabet}\n",
    "\n",
    "for m in messages:\n",
    "    for c in alphabet:\n",
    "        counter[c] += m.count(c)\n",
    "\n",
    "counter_sorted = sorted(counter, key=counter.__getitem__, reverse=True)\n",
    "\n",
    "char_to_token = {c:i+1 for i,c in enumerate(counter_sorted)}\n",
    "token_to_char = {i:c for c,i in char_to_token.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get alphabet dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_token = {c:i for i, c in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(messages, char_to_token, max_seq_length):\n",
    "    num_char = len(char_to_token)\n",
    "    messages_one_hot = np.zeros((len(messages), num_char, max_seq_length), dtype=np.float32)\n",
    "\n",
    "    for i, m in enumerate(messages):\n",
    "        for j, c in enumerate(m[:max_seq_length][::-1]):\n",
    "            try:\n",
    "                messages_one_hot[i, char_to_token[c], j] = 1.\n",
    "            except:\n",
    "                pass # unknown characters will be encoded as all zeros\n",
    "        \n",
    "    return messages_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((464736, 105, 140), (932456, 105, 140))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 140\n",
    "raw_one_hot = one_hot(messages, char_to_token, max_seq_length)\n",
    "en_one_hot = one_hot(messages_en, char_to_token, max_seq_length)\n",
    "\n",
    "del messages, messages_en\n",
    "\n",
    "raw_one_hot.shape, en_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labelled train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 25000\n",
    "\n",
    "def sample_data(x, num_samples, seed=42):\n",
    "    \"Subsample an array x to get num_samples observations\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    num_observations = len(x)\n",
    "    \n",
    "    random_permuted_indices = np.random.permutation(num_observations)\n",
    "    x = x[random_permuted_indices]\n",
    "    \n",
    "    return x[:num_samples]\n",
    "\n",
    "enc_train = sample_data(en_one_hot, num_samples)\n",
    "raw_train = sample_data(raw_one_hot, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.hstack([np.ones(num_samples, dtype=np.float32), np.zeros(num_samples, dtype=np.float32)])\n",
    "features = np.vstack([raw_train, enc_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "def split_data(x, y, test_size=0.25, seed=42):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def create_tensor_datasets(x, y, batch_size, test_size=0.25, seed=42):\n",
    "    # Split data\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, test_size=test_size, seed=seed)\n",
    "\n",
    "    # Create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 128\n",
    "\n",
    "train_loader, test_loader = create_tensor_datasets(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    A character-level CNN for text classification. \n",
    "    This architecture is inspired by Zhang et al., 2016. (Character-level Convolutional Networks for TextClassification)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alphabet_size, \n",
    "                 max_seq_length, \n",
    "                 num_classes, \n",
    "                 num_conv_filters=256,\n",
    "                 num_fc_filters=1024,\n",
    "                 conv_kernel_sizes=[7, 7, 3, 3, 3, 3],\n",
    "                 pool_kernel_sizes=[3, 3, None, None, None, 3]):\n",
    "        \n",
    "        super(CharCNN, self).__init__()\n",
    "        \n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.num_conv_filters = num_conv_filters\n",
    "        self.conv_kernel_sizes = conv_kernel_sizes\n",
    "        self.pool_kernel_sizes = pool_kernel_sizes\n",
    "        \n",
    "        # Calculate output length of last conv. layer\n",
    "        self.conv_seq_length = self._calculate_conv_seq_length()\n",
    "        \n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(self.alphabet_size, num_conv_filters, \n",
    "                                             kernel_size=7, padding=0),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(num_conv_filters, num_conv_filters, \n",
    "                                             kernel_size=7, padding=0),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3))\n",
    "        \n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(num_conv_filters, num_conv_filters, \n",
    "                                             kernel_size=3, padding=0),\n",
    "                                   nn.ReLU())\n",
    "        \n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(num_conv_filters, num_conv_filters, \n",
    "                                             kernel_size=3, padding=0),\n",
    "                                   nn.ReLU())\n",
    "        \n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(num_conv_filters, num_conv_filters, \n",
    "                                             kernel_size=3, padding=0),\n",
    "                                   nn.ReLU())\n",
    "        \n",
    "        self.conv6 = nn.Sequential(nn.Conv1d(num_conv_filters, num_conv_filters, \n",
    "                                             kernel_size=3, padding=0),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3))\n",
    "    \n",
    "        \n",
    "        # Define fully-connected output layers\n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.conv_seq_length, num_fc_filters),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.5))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(nn.Linear(num_fc_filters, num_fc_filters),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.5))\n",
    "        \n",
    "        self.fc_out = nn.Linear(num_fc_filters, self.num_classes)\n",
    "        \n",
    "        self._initialise_weights()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        # Reshape\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully-connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _calculate_conv_seq_length(self):\n",
    "        conv_seq_length = self.max_seq_length\n",
    "\n",
    "        for fc, fp in zip(self.conv_kernel_sizes, self.pool_kernel_sizes):\n",
    "            conv_seq_length = (conv_seq_length - fc) + 1\n",
    "\n",
    "            if fp is not None:\n",
    "                conv_seq_length = (conv_seq_length - fp)//fp + 1\n",
    "        \n",
    "        return conv_seq_length * self.num_conv_filters\n",
    "\n",
    "\n",
    "    def _initialise_weights(self, mean=0.0, std=0.05):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CharCNN(len(alphabet), max_seq_length, 2)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "shape = (batch_size, len(alphabet), max_seq_length)\n",
    "x = torch.rand(shape)\n",
    "\n",
    "y = cnn.forward(x)\n",
    "\n",
    "x = torch.from_numpy(features[:10])\n",
    "cnn.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, optimiser, criterion, num_epochs, print_every=500):\n",
    "    \"\"\" Training procedure\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        \n",
    "        for batch_num, batch in progress_bar:\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs.cuda()\n",
    "                labels.cuda()\n",
    "                \n",
    "            optimiser.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            loss = criterion(logits, labels.long())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimiser.step()\n",
    "            \n",
    "            if (batch_num % print_every) == 0:\n",
    "                print ('Epoch [%d/%d], Batch[%d/%d], Loss: %.4f' %(epoch+1, num_epochs, batch_num, len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cnn.cuda()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 0.01\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 2\n",
    "train(cnn, train_loader, optimiser, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, labels):\n",
    "    \n",
    "    predicted = get_labels(logits)\n",
    "    correct = predicted.eq(labels)\n",
    "    \n",
    "    return correct.sum().float() / correct.nelement()\n",
    "\n",
    "def get_labels(logits):\n",
    "    probabilities = nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    labels = torch.argmax(probabilities, 1)\n",
    "    \n",
    "    return labels\n",
    "    \n",
    "# Test\n",
    "cnn.eval()\n",
    "x = torch.from_numpy(features[:100])\n",
    "logits = cnn.forward(x)\n",
    "true = torch.from_numpy(labels[:100])\n",
    "\n",
    "a = get_accuracy(logits, true)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    validation = {'accuracy': [],\n",
    "                 'avg_loss': [],\n",
    "                 'label': [],\n",
    "                 'predicted': []}\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    \n",
    "    for batch_num, batch in progress_bar:\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)\n",
    "            predicted = get_labels(logits)\n",
    "            \n",
    "        validation['label'].append(labels.cpu().detach().numpy().flatten())\n",
    "        validation['predicted'].append(predicted.cpu().detach().numpy().flatten())\n",
    "        \n",
    "        acc = get_accuracy(logits, labels).cpu().detach().numpy()\n",
    "        validation['accuracy'].extend(list(acc.flatten()))\n",
    "            \n",
    "        loss = criterion(logits, labels.long())\n",
    "        avg_loss = torch.mean(loss.data).cpu().detach().numpy()\n",
    "        validation['avg_loss'].extend(list(avg_loss.flatten()))\n",
    "        \n",
    "    \n",
    "    return validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test = evaluate(cnn, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = evaluate(cnn, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tr = pd.DataFrame(val_train)\n",
    "val_tr['set'] = 'Train'\n",
    "\n",
    "val_te = pd.DataFrame(val_test)\n",
    "val_te['set'] = 'Test'\n",
    "\n",
    "val = pd.concat([val_tr, val_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.boxplot(x='set', y='accuracy', data=val)\n",
    "\n",
    "ax.set(title='CharCNN', ylabel='Accuracy', xlabel='');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map ={1: 'ACARS', 0: 'NON-ACARS'}\n",
    "\n",
    "def predict(messages, model, char_to_token, max_seq_length):\n",
    "    # Tokenise messages\n",
    "    inputs = one_hot(messages, char_to_token, max_seq_length)\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    logits = model(inputs)\n",
    "    predicted = get_labels(logits)\n",
    "    predicted = predicted.cpu().detach().numpy()\n",
    "    \n",
    "    print('Example messages:\\n')\n",
    "    for i, m in enumerate(messages[:5]):\n",
    "        print(f'Message: {m}     Classified as: {label_map[predicted[i]]}\\n')\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '../data/en_messages/en_plaintext_1_vigenere.txt'\n",
    "messages_test = load_txt(f)\n",
    "\n",
    "predict(messages_test[-100:], cnn, char_to_token, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '../data/raw_messages/raw_sat_plaintext_2.txt'\n",
    "messages_test = load_txt(f)\n",
    "\n",
    "predict(messages_test[-100:], cnn, char_to_token, max_seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
